{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://spark.apache.org/docs/latest/quick-start.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"Simple App\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark’s primary abstraction is a distributed collection of items called a Resilient Distributed Dataset (RDD). RDDs can be created from Hadoop InputFormats (such as HDFS files) or by transforming other RDDs. Let’s make a new RDD from the text of the README file in the Spark source directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textFile = sc.textFile(\"README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs have [actions](https://spark.apache.org/docs/latest/programming-guide.html#actions), which return values, and [transformations](https://spark.apache.org/docs/latest/programming-guide.html#transformations), which return pointers to new RDDs. Let’s start with a few actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.count()  # Number of items in this RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Apache Spark'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.first()  # First item in this RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s use a transformation. We will use the [filter](https://spark.apache.org/docs/latest/programming-guide.html#transformations) transformation to return a new RDD with a subset of the items in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linesWithSpark = textFile.filter(lambda line: \"Spark\" in line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can chain together transformations and actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.filter(lambda line: \"Spark\" in line).count()  # How many lines contain \"Spark\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on RDD Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD actions and transformations can be used for more complex computations. Let’s say we want to find the line with the most words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.map(lambda line: len(line.split())).reduce(lambda a, b: a if (a > b) else b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first maps a line to an integer value, creating a new RDD. reduce is called on that RDD to find the largest line count. The arguments to map and reduce are Python [anonymous functions (lambdas)](https://docs.python.org/2/reference/expressions.html#lambda), but we can also pass any top-level Python function we want. For example, we’ll define a max function to make this code easier to understand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max(a, b):\n",
    "    return a if (a > b) else b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.map(lambda line: len(line.split())).reduce(max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common data flow pattern is MapReduce, as popularized by Hadoop. Spark can implement MapReduce flows easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordCounts = textFile.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we combined the [flatMap](https://spark.apache.org/docs/latest/programming-guide.html#transformations), [map](https://spark.apache.org/docs/latest/programming-guide.html#transformations), and [reduceByKey](https://spark.apache.org/docs/latest/programming-guide.html#transformations) transformations to compute the per-word counts in the file as an RDD of (string, int) pairs. To collect the word counts in our shell, we can use the [collect](https://spark.apache.org/docs/latest/programming-guide.html#actions) action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('guide,', 1),\n",
       " ('APIs', 1),\n",
       " ('standalone,', 1),\n",
       " ('for', 8),\n",
       " ('At', 1),\n",
       " ('we', 1),\n",
       " ('tools', 2),\n",
       " ('please', 1),\n",
       " ('depends', 1),\n",
       " ('find', 1),\n",
       " ('matches', 1),\n",
       " ('computing', 1),\n",
       " ('Packaging', 1),\n",
       " ('replace', 1),\n",
       " ('rich', 1),\n",
       " ('0.10.4),', 1),\n",
       " ('graph', 1),\n",
       " ('you', 4),\n",
       " ('The', 1),\n",
       " ('programming', 1),\n",
       " ('Online', 1),\n",
       " ('a', 4),\n",
       " ('change', 1),\n",
       " ('an', 2),\n",
       " ('numpy', 1),\n",
       " ('with', 2),\n",
       " ('may', 2),\n",
       " ('JARs,', 1),\n",
       " ('[project', 1),\n",
       " ('Data.', 1),\n",
       " ('their', 1),\n",
       " ('version', 4),\n",
       " ('if', 1),\n",
       " ('README', 1),\n",
       " ('PySpark.', 1),\n",
       " ('file', 1),\n",
       " ('related', 1),\n",
       " ('[Apache', 1),\n",
       " ('-', 1),\n",
       " ('MLlib', 1),\n",
       " ('that', 2),\n",
       " ('source', 1),\n",
       " ('contain', 1),\n",
       " ('latest', 1),\n",
       " ('to', 4),\n",
       " ('not', 2),\n",
       " ('minor', 1),\n",
       " ('Documentation', 1),\n",
       " ('general', 2),\n",
       " ('own', 2),\n",
       " ('fast', 1),\n",
       " ('high-level', 1),\n",
       " ('YARN,', 1),\n",
       " ('downloads', 1),\n",
       " ('this', 2),\n",
       " ('information', 1),\n",
       " ('cluster', 3),\n",
       " ('setup', 1),\n",
       " ('machine', 1),\n",
       " ('packaged', 1),\n",
       " ('download', 1),\n",
       " ('(be', 1),\n",
       " ('full', 1),\n",
       " ('This', 3),\n",
       " ('builder', 1),\n",
       " ('Python,', 1),\n",
       " ('Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1),\n",
       " ('other', 1),\n",
       " ('use', 1),\n",
       " ('suitable', 1),\n",
       " ('Streaming', 1),\n",
       " ('**NOTE:**', 1),\n",
       " ('cases.', 1),\n",
       " ('set', 1),\n",
       " ('standalone', 2),\n",
       " ('at', 1),\n",
       " ('system', 1),\n",
       " ('processing,', 1),\n",
       " ('learning,', 1),\n",
       " ('pip', 1),\n",
       " ('including', 2),\n",
       " ('ensure', 1),\n",
       " ('computation', 1),\n",
       " ('Big', 1),\n",
       " ('experience', 1),\n",
       " ('documentation,', 1),\n",
       " ('using', 1),\n",
       " ('currently', 1),\n",
       " ('(although', 1),\n",
       " ('additional', 1),\n",
       " ('It', 2),\n",
       " ('our', 1),\n",
       " ('requirements', 1),\n",
       " ('its', 1),\n",
       " ('but', 2),\n",
       " ('keep', 1),\n",
       " ('your', 1),\n",
       " ('must', 1),\n",
       " ('the', 9),\n",
       " ('Mesos)', 1),\n",
       " ('it', 1),\n",
       " ('SQL', 2),\n",
       " ('You', 2),\n",
       " ('core', 1),\n",
       " ('Scala,', 1),\n",
       " ('instructions', 1),\n",
       " ('processing.', 1),\n",
       " ('required', 1),\n",
       " ('does', 1),\n",
       " ('have', 1),\n",
       " ('and', 8),\n",
       " ('or', 2),\n",
       " ('version)', 1),\n",
       " ('higher-level', 1),\n",
       " ('provides', 1),\n",
       " ('Python', 4),\n",
       " ('cluster.', 1),\n",
       " ('also', 1),\n",
       " ('in', 2),\n",
       " ('see', 1),\n",
       " ('best', 1),\n",
       " ('page](http://spark.apache.org/documentation.html)', 1),\n",
       " ('supports', 2),\n",
       " ('PySpark', 2),\n",
       " ('sub-packages', 1),\n",
       " ('odd', 1),\n",
       " ('compatibility).', 1),\n",
       " ('are', 2),\n",
       " ('Using', 1),\n",
       " ('installed', 1),\n",
       " ('will', 1),\n",
       " ('from', 2),\n",
       " ('GraphX', 1),\n",
       " ('packaging', 2),\n",
       " ('DataFrames,', 1),\n",
       " ('[\"Building', 1),\n",
       " ('#', 1),\n",
       " ('pandas).', 1),\n",
       " ('interacting', 1),\n",
       " ('engine', 1),\n",
       " ('analysis.', 1),\n",
       " ('intended', 1),\n",
       " ('of', 4),\n",
       " ('only', 1),\n",
       " ('web', 1),\n",
       " ('page](http://spark.apache.org/downloads.html).', 1),\n",
       " ('(currently', 1),\n",
       " ('Py4J', 1),\n",
       " ('basic', 1),\n",
       " ('data', 1),\n",
       " ('do', 1),\n",
       " ('on', 2),\n",
       " ('optimized', 1),\n",
       " ('future', 1),\n",
       " ('Apache', 1),\n",
       " ('building', 1),\n",
       " ('Requirements', 1),\n",
       " ('requires', 1),\n",
       " ('(including', 2),\n",
       " ('##', 3),\n",
       " ('R,', 1),\n",
       " ('Java,', 1),\n",
       " ('errors.', 1),\n",
       " ('If', 1),\n",
       " ('<http://spark.apache.org/>', 1),\n",
       " ('contains', 1),\n",
       " ('graphs', 1),\n",
       " ('stream', 1),\n",
       " ('Spark', 13),\n",
       " ('all', 1),\n",
       " ('is', 4),\n",
       " ('existing', 1),\n",
       " ('experimental', 1),\n",
       " ('versions', 1),\n",
       " ('can', 2)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Spark also supports pulling data sets into a cluster-wide in-memory cache.\n",
    "# This is very useful when data is accessed repeatedly, such as when querying a small “hot” dataset or when running an iterative algorithm like PageRank.\n",
    "# As a simple example, let’s mark our linesWithSpark dataset to be cached:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[12] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesWithSpark.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesWithSpark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesWithSpark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It may seem silly to use Spark to explore and cache a 100-line text file.\n",
    "# The interesting part is that these same functions can be used on very large data sets, even when they are striped across tens or hundreds of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
